{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis Experiment Notebook (Part A + Part B MVP)\n",
    "\n",
    "This notebook runs a minimal end-to-end experiment for finite-sample Sharpe inference under dependence/heavy tails.\n",
    "\n",
    "Scope:\n",
    "- Part A: Monte Carlo calibration on 3 DGPs (`iid_normal`, standardized `iid_t_nu`, `garch11_t`) for `n in {120, 240, 1200}` and `S_true in {0.0, 0.5}`.\n",
    "- Part B (MVP): empirical PIT adequacy on holdout disjoint windows at `n=240` for 3 fitted models.\n",
    "\n",
    "Produced files (saved under `outputs/thesis_mvp/`):\n",
    "- `results_partA.parquet` (or CSV fallback)\n",
    "- `results_partB_pit.parquet` (or CSV fallback)\n",
    "- `model_fit_summary.json`\n",
    "- `environment_versions.json` and `.txt`\n",
    "- Figures (`.png` + `.pdf`) in `outputs/thesis_mvp/figures/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import platform\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib.metadata as ilmd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(iterable=None, *args, **kwargs):\n",
    "        return iterable if iterable is not None else []\n",
    "    print(\"tqdm not installed; using no-op progress wrapper.\")\n",
    "from scipy import stats\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.6f}\")\n",
    "\n",
    "RUN_ROOT = Path(\"outputs/thesis_mvp\").resolve()\n",
    "RUN_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SAVED_ARTIFACTS = []\n",
    "\n",
    "\n",
    "def _pkg_version(name: str) -> str:\n",
    "    try:\n",
    "        return ilmd.version(name)\n",
    "    except Exception:\n",
    "        return \"not-installed\"\n",
    "\n",
    "\n",
    "version_payload = {\n",
    "    \"python\": sys.version,\n",
    "    \"platform\": platform.platform(),\n",
    "    \"packages\": {\n",
    "        \"numpy\": _pkg_version(\"numpy\"),\n",
    "        \"pandas\": _pkg_version(\"pandas\"),\n",
    "        \"scipy\": _pkg_version(\"scipy\"),\n",
    "        \"matplotlib\": _pkg_version(\"matplotlib\"),\n",
    "        \"tqdm\": _pkg_version(\"tqdm\"),\n",
    "        \"arch\": _pkg_version(\"arch\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Environment versions:\")\n",
    "print(json.dumps(version_payload, indent=2))\n",
    "\n",
    "versions_json_path = RUN_ROOT / \"environment_versions.json\"\n",
    "versions_txt_path = RUN_ROOT / \"environment_versions.txt\"\n",
    "versions_json_path.write_text(json.dumps(version_payload, indent=2), encoding=\"utf-8\")\n",
    "versions_txt_path.write_text(json.dumps(version_payload, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "SAVED_ARTIFACTS.extend([str(versions_json_path), str(versions_txt_path)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"R\": 5000,\n",
    "    \"seed\": 12345,\n",
    "    \"n_list\": [120, 240, 1200],\n",
    "    \"S_true_list\": [0.0, 0.5],\n",
    "    \"alpha\": 0.05,\n",
    "    \"burn_B\": 500,\n",
    "    \"t_df\": 5.0,\n",
    "    \"garch_alpha\": 0.05,\n",
    "    \"garch_beta\": 0.90,\n",
    "    \"DSR_M\": None,\n",
    "    \"cache_dir\": str((RUN_ROOT / \"cache\").resolve()),\n",
    "    \"figures_dir\": str((RUN_ROOT / \"figures\").resolve()),\n",
    "    \"data_path\": \"data/monthly_excess_returns.csv\",\n",
    "}\n",
    "\n",
    "if CONFIG[\"t_df\"] <= 4:\n",
    "    raise ValueError(\"CONFIG['t_df'] must be > 4 for finite higher moments used by inference methods.\")\n",
    "if CONFIG[\"garch_alpha\"] + CONFIG[\"garch_beta\"] >= 1:\n",
    "    raise ValueError(\"Need garch_alpha + garch_beta < 1 for unconditional variance stationarity.\")\n",
    "\n",
    "Path(CONFIG[\"cache_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG[\"figures_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Config:\")\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to sys.path and import existing project modules.\n",
    "cwd = Path.cwd().resolve()\n",
    "PROJECT_ROOT = None\n",
    "for p in [cwd, *cwd.parents]:\n",
    "    if (p / \"src\" / \"sharpe_mc.py\").exists():\n",
    "        PROJECT_ROOT = p\n",
    "        if str(p) not in sys.path:\n",
    "            sys.path.insert(0, str(p))\n",
    "        break\n",
    "\n",
    "if PROJECT_ROOT is None:\n",
    "    raise RuntimeError(\"Could not locate project root containing src/sharpe_mc.py\")\n",
    "\n",
    "from src import sharpe_mc\n",
    "\n",
    "PROJECT_NOTES = []\n",
    "\n",
    "required_symbols = [\n",
    "    \"simulate_iid_normal\",\n",
    "    \"simulate_iid_t5\",\n",
    "    \"simulate_garch11_t5\",\n",
    "    \"sharpe_ratio\",\n",
    "    \"se_naive\",\n",
    "    \"se_hac\",\n",
    "    \"psr_probability\",\n",
    "]\n",
    "missing = [name for name in required_symbols if not hasattr(sharpe_mc, name)]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing required symbols in src/sharpe_mc.py: {missing}\")\n",
    "\n",
    "if not hasattr(sharpe_mc, \"dsr_probability\"):\n",
    "    PROJECT_NOTES.append(\"DSR function missing in project; DSR will be skipped.\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "if PROJECT_NOTES:\n",
    "    print(\"Project notes:\")\n",
    "    for note in PROJECT_NOTES:\n",
    "        print(\"-\", note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = Path(CONFIG[\"cache_dir\"])\n",
    "FIGURES_DIR = Path(CONFIG[\"figures_dir\"])\n",
    "RESULTS_DIR = RUN_ROOT\n",
    "\n",
    "OUTPUT_COLS = [\n",
    "    \"dgp\",\n",
    "    \"n\",\n",
    "    \"S_true\",\n",
    "    \"method\",\n",
    "    \"bias\",\n",
    "    \"rmse\",\n",
    "    \"coverage_95\",\n",
    "    \"reject_rate_H0_S_le_0\",\n",
    "    \"se_ratio\",\n",
    "    \"psr_reject_rate\",\n",
    "    \"dsr_reject_rate\",\n",
    "]\n",
    "\n",
    "\n",
    "def _track_artifact(path: Path) -> Path:\n",
    "    path = Path(path).resolve()\n",
    "    SAVED_ARTIFACTS.append(str(path))\n",
    "    return path\n",
    "\n",
    "\n",
    "def set_global_seed(seed: int) -> None:\n",
    "    random.seed(int(seed))\n",
    "    np.random.seed(int(seed))\n",
    "\n",
    "\n",
    "def _safe_tag(x: float) -> str:\n",
    "    return str(x).replace(\"-\", \"m\").replace(\".\", \"p\")\n",
    "\n",
    "\n",
    "def series_checksum(arr: np.ndarray) -> str:\n",
    "    a = np.asarray(arr, dtype=np.float64)\n",
    "    return hashlib.sha256(a.tobytes()).hexdigest()[:12]\n",
    "\n",
    "\n",
    "def cache_save_df(df: pd.DataFrame, stem: str) -> Path:\n",
    "    stem = stem.strip()\n",
    "    pq = CACHE_DIR / f\"{stem}.parquet\"\n",
    "    csv = CACHE_DIR / f\"{stem}.csv\"\n",
    "    try:\n",
    "        df.to_parquet(pq, index=False)\n",
    "        return pq\n",
    "    except Exception:\n",
    "        df.to_csv(csv, index=False)\n",
    "        return csv\n",
    "\n",
    "\n",
    "def cache_load_df(stem: str) -> pd.DataFrame | None:\n",
    "    stem = stem.strip()\n",
    "    pq = CACHE_DIR / f\"{stem}.parquet\"\n",
    "    csv = CACHE_DIR / f\"{stem}.csv\"\n",
    "    if pq.exists():\n",
    "        try:\n",
    "            return pd.read_parquet(pq)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if csv.exists():\n",
    "        return pd.read_csv(csv)\n",
    "    return None\n",
    "\n",
    "\n",
    "def result_save_df(df: pd.DataFrame, base_name: str) -> list[Path]:\n",
    "    out_paths: list[Path] = []\n",
    "    pq = RESULTS_DIR / f\"{base_name}.parquet\"\n",
    "    csv = RESULTS_DIR / f\"{base_name}.csv\"\n",
    "    try:\n",
    "        df.to_parquet(pq, index=False)\n",
    "        out_paths.append(_track_artifact(pq))\n",
    "    except Exception:\n",
    "        pass\n",
    "    df.to_csv(csv, index=False)\n",
    "    out_paths.append(_track_artifact(csv))\n",
    "    return out_paths\n",
    "\n",
    "\n",
    "def cache_save_array(arr: np.ndarray, stem: str) -> Path:\n",
    "    path = CACHE_DIR / f\"{stem}.npy\"\n",
    "    np.save(path, np.asarray(arr, dtype=np.float64))\n",
    "    return path\n",
    "\n",
    "\n",
    "def cache_load_array(stem: str) -> np.ndarray | None:\n",
    "    path = CACHE_DIR / f\"{stem}.npy\"\n",
    "    if path.exists():\n",
    "        return np.load(path)\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_figure(fig: plt.Figure, stem: str) -> tuple[Path, Path]:\n",
    "    png_path = FIGURES_DIR / f\"{stem}.png\"\n",
    "    pdf_path = FIGURES_DIR / f\"{stem}.pdf\"\n",
    "    fig.savefig(png_path, dpi=160, bbox_inches=\"tight\")\n",
    "    fig.savefig(pdf_path, bbox_inches=\"tight\")\n",
    "    _track_artifact(png_path)\n",
    "    _track_artifact(pdf_path)\n",
    "    return png_path, pdf_path\n",
    "\n",
    "\n",
    "def run_methods_on_sample(sample: np.ndarray, S_true: float, n: int) -> dict:\n",
    "    \"\"\"Run all requested inference methods on one sample and return per-method scalars.\"\"\"\n",
    "    x = np.asarray(sample, dtype=np.float64)\n",
    "    alpha = float(CONFIG[\"alpha\"])\n",
    "    z_two = float(stats.norm.ppf(1.0 - alpha / 2.0))\n",
    "    z_one = float(stats.norm.ppf(1.0 - alpha))\n",
    "\n",
    "    S_hat, _, _ = sharpe_mc.sharpe_ratio(x)\n",
    "\n",
    "    se_iid = float(sharpe_mc.se_naive(S_hat, n))\n",
    "    ci_iid_low = float(S_hat - z_two * se_iid)\n",
    "    ci_iid_high = float(S_hat + z_two * se_iid)\n",
    "    cover_iid = bool(ci_iid_low <= S_true <= ci_iid_high)\n",
    "    reject_iid = bool((S_hat / se_iid) > z_one)\n",
    "\n",
    "    se_hac = float(sharpe_mc.se_hac(x, S_hat))\n",
    "    ci_hac_low = float(S_hat - z_two * se_hac)\n",
    "    ci_hac_high = float(S_hat + z_two * se_hac)\n",
    "    cover_hac = bool(ci_hac_low <= S_true <= ci_hac_high)\n",
    "    reject_hac = bool((S_hat / se_hac) > z_one)\n",
    "\n",
    "    skew_x = float(stats.skew(x, bias=False))\n",
    "    kurt_x = float(stats.kurtosis(x, fisher=False, bias=False))\n",
    "\n",
    "    psr_prob = float(sharpe_mc.psr_probability(S_hat, skew_x, kurt_x, n, S_ref=0.0))\n",
    "    psr_reject = bool(psr_prob > (1.0 - alpha))\n",
    "\n",
    "    dsr_prob = np.nan\n",
    "    dsr_reject = np.nan\n",
    "    if CONFIG[\"DSR_M\"] is not None and hasattr(sharpe_mc, \"dsr_probability\"):\n",
    "        dsr_prob = float(\n",
    "            sharpe_mc.dsr_probability(\n",
    "                S_hat,\n",
    "                skew_x,\n",
    "                kurt_x,\n",
    "                n,\n",
    "                int(CONFIG[\"DSR_M\"]),\n",
    "                S_ref=0.0,\n",
    "            )\n",
    "        )\n",
    "        dsr_reject = bool(dsr_prob > (1.0 - alpha))\n",
    "\n",
    "    return {\n",
    "        \"S_hat\": float(S_hat),\n",
    "        \"iid_normal_analytic\": {\n",
    "            \"ci_low\": ci_iid_low,\n",
    "            \"ci_high\": ci_iid_high,\n",
    "            \"coverage\": cover_iid,\n",
    "            \"reject\": reject_iid,\n",
    "            \"se\": se_iid,\n",
    "        },\n",
    "        \"hac_newey_west\": {\n",
    "            \"ci_low\": ci_hac_low,\n",
    "            \"ci_high\": ci_hac_high,\n",
    "            \"coverage\": cover_hac,\n",
    "            \"reject\": reject_hac,\n",
    "            \"se\": se_hac,\n",
    "        },\n",
    "        \"psr\": {\n",
    "            \"ci_low\": np.nan,\n",
    "            \"ci_high\": np.nan,\n",
    "            \"coverage\": np.nan,\n",
    "            \"reject\": psr_reject,\n",
    "            \"se\": np.nan,\n",
    "            \"prob\": psr_prob,\n",
    "        },\n",
    "        \"dsr\": {\n",
    "            \"ci_low\": np.nan,\n",
    "            \"ci_high\": np.nan,\n",
    "            \"coverage\": np.nan,\n",
    "            \"reject\": dsr_reject,\n",
    "            \"se\": np.nan,\n",
    "            \"prob\": dsr_prob,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def simulate_partA_cell(dgp: str, n: int, S_true: float, R: int, seed: int) -> pd.DataFrame:\n",
    "    \"\"\"Simulate one (dgp, n, S_true) cell and store per-replication scalars only.\"\"\"\n",
    "    set_global_seed(seed)\n",
    "    rng = np.random.default_rng(seed + 100003)\n",
    "\n",
    "    S_hat = np.empty(R, dtype=np.float64)\n",
    "\n",
    "    se_iid = np.empty(R, dtype=np.float64)\n",
    "    cover_iid = np.empty(R, dtype=bool)\n",
    "    reject_iid = np.empty(R, dtype=bool)\n",
    "\n",
    "    se_hac = np.empty(R, dtype=np.float64)\n",
    "    cover_hac = np.empty(R, dtype=bool)\n",
    "    reject_hac = np.empty(R, dtype=bool)\n",
    "\n",
    "    psr_reject = np.empty(R, dtype=bool)\n",
    "    dsr_reject = np.full(R, np.nan, dtype=np.float64)\n",
    "\n",
    "    ci_iid_low = np.empty(R, dtype=np.float64)\n",
    "    ci_iid_high = np.empty(R, dtype=np.float64)\n",
    "    ci_hac_low = np.empty(R, dtype=np.float64)\n",
    "    ci_hac_high = np.empty(R, dtype=np.float64)\n",
    "\n",
    "    for i in tqdm(range(R), desc=f\"{dgp} | n={n} | S={S_true}\", leave=False):\n",
    "        if dgp == \"iid_normal\":\n",
    "            sample = sharpe_mc.simulate_iid_normal(n=n, mu=S_true, sigma=1.0, rng=rng)\n",
    "        elif dgp == \"iid_t\":\n",
    "            sample = sharpe_mc.simulate_iid_t5(\n",
    "                n=n,\n",
    "                mu=S_true,\n",
    "                sigma=1.0,\n",
    "                rng=rng,\n",
    "                df=int(CONFIG[\"t_df\"]),\n",
    "            )\n",
    "        elif dgp == \"garch11_t\":\n",
    "            sample = sharpe_mc.simulate_garch11_t5(\n",
    "                n=n,\n",
    "                mu=S_true,\n",
    "                sigma=1.0,\n",
    "                alpha=float(CONFIG[\"garch_alpha\"]),\n",
    "                beta=float(CONFIG[\"garch_beta\"]),\n",
    "                df=int(CONFIG[\"t_df\"]),\n",
    "                burn=int(CONFIG[\"burn_B\"]),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dgp: {dgp}\")\n",
    "\n",
    "        out = run_methods_on_sample(sample=sample, S_true=S_true, n=n)\n",
    "        S_hat[i] = out[\"S_hat\"]\n",
    "\n",
    "        iid = out[\"iid_normal_analytic\"]\n",
    "        se_iid[i] = iid[\"se\"]\n",
    "        cover_iid[i] = iid[\"coverage\"]\n",
    "        reject_iid[i] = iid[\"reject\"]\n",
    "        ci_iid_low[i] = iid[\"ci_low\"]\n",
    "        ci_iid_high[i] = iid[\"ci_high\"]\n",
    "\n",
    "        hac = out[\"hac_newey_west\"]\n",
    "        se_hac[i] = hac[\"se\"]\n",
    "        cover_hac[i] = hac[\"coverage\"]\n",
    "        reject_hac[i] = hac[\"reject\"]\n",
    "        ci_hac_low[i] = hac[\"ci_low\"]\n",
    "        ci_hac_high[i] = hac[\"ci_high\"]\n",
    "\n",
    "        psr_reject[i] = out[\"psr\"][\"reject\"]\n",
    "\n",
    "        if CONFIG[\"DSR_M\"] is not None and hasattr(sharpe_mc, \"dsr_probability\"):\n",
    "            dsr_reject[i] = float(out[\"dsr\"][\"reject\"])\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"S_hat\": S_hat,\n",
    "            \"se_iid\": se_iid,\n",
    "            \"se_hac\": se_hac,\n",
    "            \"cover_iid\": cover_iid,\n",
    "            \"cover_hac\": cover_hac,\n",
    "            \"reject_iid\": reject_iid,\n",
    "            \"reject_hac\": reject_hac,\n",
    "            \"psr_reject\": psr_reject,\n",
    "            \"dsr_reject\": dsr_reject,\n",
    "            \"ci_iid_low\": ci_iid_low,\n",
    "            \"ci_iid_high\": ci_iid_high,\n",
    "            \"ci_hac_low\": ci_hac_low,\n",
    "            \"ci_hac_high\": ci_hac_high,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def summarize_results_partA(rep_df: pd.DataFrame, dgp: str, n: int, S_true: float) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate one cell to required output schema.\n",
    "\n",
    "    bias and rmse are computed once from S_hat and replicated across method rows.\n",
    "    \"\"\"\n",
    "    s = rep_df[\"S_hat\"].to_numpy(dtype=np.float64)\n",
    "    bias = float(np.mean(s) - S_true)\n",
    "    rmse = float(np.sqrt(np.mean((s - S_true) ** 2)))\n",
    "\n",
    "    sd_emp = float(np.std(s, ddof=1))\n",
    "\n",
    "    psr_rate = float(np.mean(rep_df[\"psr_reject\"])) if len(rep_df) else np.nan\n",
    "    dsr_available = bool(np.isfinite(rep_df[\"dsr_reject\"]).any())\n",
    "    dsr_rate = float(np.nanmean(rep_df[\"dsr_reject\"])) if dsr_available else np.nan\n",
    "\n",
    "    se_ratio_iid = float(np.mean(rep_df[\"se_iid\"]) / sd_emp) if sd_emp > 0 else np.nan\n",
    "    se_ratio_hac = float(np.mean(rep_df[\"se_hac\"]) / sd_emp) if sd_emp > 0 else np.nan\n",
    "\n",
    "    rows = [\n",
    "        {\n",
    "            \"dgp\": dgp,\n",
    "            \"n\": int(n),\n",
    "            \"S_true\": float(S_true),\n",
    "            \"method\": \"iid_normal_analytic\",\n",
    "            \"bias\": bias,\n",
    "            \"rmse\": rmse,\n",
    "            \"coverage_95\": float(np.mean(rep_df[\"cover_iid\"])),\n",
    "            \"reject_rate_H0_S_le_0\": float(np.mean(rep_df[\"reject_iid\"])),\n",
    "            \"se_ratio\": se_ratio_iid,\n",
    "            \"psr_reject_rate\": psr_rate,\n",
    "            \"dsr_reject_rate\": dsr_rate,\n",
    "        },\n",
    "        {\n",
    "            \"dgp\": dgp,\n",
    "            \"n\": int(n),\n",
    "            \"S_true\": float(S_true),\n",
    "            \"method\": \"hac_newey_west\",\n",
    "            \"bias\": bias,\n",
    "            \"rmse\": rmse,\n",
    "            \"coverage_95\": float(np.mean(rep_df[\"cover_hac\"])),\n",
    "            \"reject_rate_H0_S_le_0\": float(np.mean(rep_df[\"reject_hac\"])),\n",
    "            \"se_ratio\": se_ratio_hac,\n",
    "            \"psr_reject_rate\": psr_rate,\n",
    "            \"dsr_reject_rate\": dsr_rate,\n",
    "        },\n",
    "        {\n",
    "            \"dgp\": dgp,\n",
    "            \"n\": int(n),\n",
    "            \"S_true\": float(S_true),\n",
    "            \"method\": \"psr\",\n",
    "            \"bias\": bias,\n",
    "            \"rmse\": rmse,\n",
    "            \"coverage_95\": np.nan,\n",
    "            \"reject_rate_H0_S_le_0\": psr_rate,\n",
    "            \"se_ratio\": np.nan,\n",
    "            \"psr_reject_rate\": psr_rate,\n",
    "            \"dsr_reject_rate\": dsr_rate,\n",
    "        },\n",
    "        {\n",
    "            \"dgp\": dgp,\n",
    "            \"n\": int(n),\n",
    "            \"S_true\": float(S_true),\n",
    "            \"method\": \"dsr\",\n",
    "            \"bias\": bias,\n",
    "            \"rmse\": rmse,\n",
    "            \"coverage_95\": np.nan,\n",
    "            \"reject_rate_H0_S_le_0\": dsr_rate if dsr_available else np.nan,\n",
    "            \"se_ratio\": np.nan,\n",
    "            \"psr_reject_rate\": psr_rate,\n",
    "            \"dsr_reject_rate\": dsr_rate,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return pd.DataFrame(rows, columns=OUTPUT_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A: run (with cache reuse if present)\n",
    "PARTA_DGPS = [\"iid_normal\", \"iid_t\", \"garch11_t\"]\n",
    "R = int(CONFIG[\"R\"])\n",
    "\n",
    "partA_grid = [\n",
    "    (dgp, int(n), float(s_true))\n",
    "    for dgp in PARTA_DGPS\n",
    "    for n in CONFIG[\"n_list\"]\n",
    "    for s_true in CONFIG[\"S_true_list\"]\n",
    "]\n",
    "\n",
    "summary_frames: list[pd.DataFrame] = []\n",
    "partA_cell_cache = []\n",
    "\n",
    "if CONFIG[\"DSR_M\"] is None:\n",
    "    print(\"DSR skipped by default (CONFIG['DSR_M']=None); DSR metrics are filled with NaN.\")\n",
    "\n",
    "for idx, (dgp, n, S_true) in enumerate(tqdm(partA_grid, desc=\"Part A cells\")):\n",
    "    cell_seed = int(CONFIG[\"seed\"] + 1009 * (idx + 1))\n",
    "    cache_stem = (\n",
    "        f\"partA_scalars_{dgp}_n{n}_S{_safe_tag(S_true)}_\"\n",
    "        f\"R{R}_seed{cell_seed}_nu{_safe_tag(CONFIG['t_df'])}_\"\n",
    "        f\"a{_safe_tag(CONFIG['garch_alpha'])}_b{_safe_tag(CONFIG['garch_beta'])}_\"\n",
    "        f\"burn{int(CONFIG['burn_B'])}_alpha{_safe_tag(CONFIG['alpha'])}_\"\n",
    "        f\"dsr{CONFIG['DSR_M']}\"\n",
    "    )\n",
    "\n",
    "    rep_df = cache_load_df(cache_stem)\n",
    "    if rep_df is None:\n",
    "        rep_df = simulate_partA_cell(\n",
    "            dgp=dgp,\n",
    "            n=n,\n",
    "            S_true=S_true,\n",
    "            R=R,\n",
    "            seed=cell_seed,\n",
    "        )\n",
    "        cache_path = cache_save_df(rep_df, cache_stem)\n",
    "    else:\n",
    "        cache_path = CACHE_DIR / (cache_stem + (\".parquet\" if (CACHE_DIR / f\"{cache_stem}.parquet\").exists() else \".csv\"))\n",
    "\n",
    "    partA_cell_cache.append(str(cache_path))\n",
    "    summary_frames.append(summarize_results_partA(rep_df=rep_df, dgp=dgp, n=n, S_true=S_true))\n",
    "\n",
    "results_partA = pd.concat(summary_frames, ignore_index=True)[OUTPUT_COLS]\n",
    "results_partA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A: save result table artifacts\n",
    "partA_paths = result_save_df(results_partA, \"results_partA\")\n",
    "print(\"Saved Part A results:\")\n",
    "for p in partA_paths:\n",
    "    print(\"-\", p)\n",
    "\n",
    "print(\"\\nPart A head:\")\n",
    "print(results_partA.head(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A sanity checks\n",
    "sanity_rows = []\n",
    "\n",
    "# 1) iid normal coverage near 95% at n=1200, S_true=0\n",
    "cov_row = results_partA[\n",
    "    (results_partA[\"dgp\"] == \"iid_normal\")\n",
    "    & (results_partA[\"n\"] == 1200)\n",
    "    & (results_partA[\"S_true\"] == 0.0)\n",
    "    & (results_partA[\"method\"] == \"iid_normal_analytic\")\n",
    "]\n",
    "if len(cov_row) == 1:\n",
    "    cov_val = float(cov_row.iloc[0][\"coverage_95\"])\n",
    "    cov_ok = abs(cov_val - 0.95) <= 0.02\n",
    "    sanity_rows.append({\"check\": \"iid normal coverage @ n=1200\", \"value\": cov_val, \"pass\": cov_ok})\n",
    "else:\n",
    "    sanity_rows.append({\"check\": \"iid normal coverage @ n=1200\", \"value\": np.nan, \"pass\": False})\n",
    "\n",
    "# 2) standardized t innovations variance close to 1\n",
    "rng_t = np.random.default_rng(int(CONFIG[\"seed\"]) + 555)\n",
    "big_t = sharpe_mc.simulate_iid_t5(\n",
    "    n=200_000,\n",
    "    mu=0.0,\n",
    "    sigma=1.0,\n",
    "    rng=rng_t,\n",
    "    df=int(CONFIG[\"t_df\"]),\n",
    ")\n",
    "var_t = float(np.var(big_t, ddof=1))\n",
    "var_ok = abs(var_t - 1.0) <= 0.08\n",
    "sanity_rows.append({\"check\": \"standardized t var ~= 1\", \"value\": var_t, \"pass\": var_ok})\n",
    "\n",
    "# 3) GARCH variance recursion positivity\n",
    "set_global_seed(int(CONFIG[\"seed\"]) + 777)\n",
    "nu = float(CONFIG[\"t_df\"])\n",
    "alpha_g = float(CONFIG[\"garch_alpha\"])\n",
    "beta_g = float(CONFIG[\"garch_beta\"])\n",
    "omega_g = 1.0 - alpha_g - beta_g\n",
    "total = 2000\n",
    "z = np.random.standard_t(df=nu, size=total) / np.sqrt(nu / (nu - 2.0))\n",
    "h = np.empty(total, dtype=np.float64)\n",
    "eps_prev = 0.0\n",
    "h_prev = 1.0\n",
    "for t in range(total):\n",
    "    h_t = omega_g + alpha_g * (eps_prev ** 2) + beta_g * h_prev\n",
    "    h_t = max(h_t, 1e-14)\n",
    "    eps_t = np.sqrt(h_t) * z[t]\n",
    "    h[t] = h_t\n",
    "    h_prev = h_t\n",
    "    eps_prev = eps_t\n",
    "\n",
    "min_h = float(np.min(h))\n",
    "sanity_rows.append({\"check\": \"garch sigma_t^2 > 0\", \"value\": min_h, \"pass\": bool(min_h > 0.0)})\n",
    "\n",
    "sanity_df = pd.DataFrame(sanity_rows)\n",
    "print(sanity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A plots: coverage vs n by method for each DGP\n",
    "method_order = [\"iid_normal_analytic\", \"hac_newey_west\", \"psr\", \"dsr\"]\n",
    "\n",
    "for dgp in PARTA_DGPS:\n",
    "    s_values = list(CONFIG[\"S_true_list\"])\n",
    "    fig, axes = plt.subplots(1, len(s_values), figsize=(6 * len(s_values), 4), sharey=True)\n",
    "    if len(s_values) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, s_true in zip(axes, s_values):\n",
    "        sub = results_partA[(results_partA[\"dgp\"] == dgp) & (results_partA[\"S_true\"] == float(s_true))]\n",
    "        for method in method_order:\n",
    "            s = sub[sub[\"method\"] == method].sort_values(\"n\")\n",
    "            y = s[\"coverage_95\"].to_numpy(dtype=float)\n",
    "            if np.isfinite(y).any():\n",
    "                ax.plot(s[\"n\"], y, marker=\"o\", label=method)\n",
    "\n",
    "        ax.axhline(0.95, color=\"black\", linestyle=\"--\", linewidth=1.0, alpha=0.8)\n",
    "        ax.set_title(f\"{dgp} | S_true={s_true}\")\n",
    "        ax.set_xlabel(\"n\")\n",
    "        ax.set_ylabel(\"coverage_95\")\n",
    "        ax.set_xticks(CONFIG[\"n_list\"])\n",
    "        ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    if handles:\n",
    "        fig.legend(handles, labels, loc=\"upper center\", ncol=min(4, len(labels)))\n",
    "    fig.suptitle(f\"Part A Coverage vs n ({dgp})\", y=1.05)\n",
    "    fig.tight_layout()\n",
    "    save_figure(fig, f\"partA_coverage_vs_n_{dgp}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"Saved Part A coverage figures.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A plots: reject rate vs n by method for each DGP\n",
    "method_order = [\"iid_normal_analytic\", \"hac_newey_west\", \"psr\", \"dsr\"]\n",
    "\n",
    "for dgp in PARTA_DGPS:\n",
    "    s_values = list(CONFIG[\"S_true_list\"])\n",
    "    fig, axes = plt.subplots(1, len(s_values), figsize=(6 * len(s_values), 4), sharey=True)\n",
    "    if len(s_values) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, s_true in zip(axes, s_values):\n",
    "        sub = results_partA[(results_partA[\"dgp\"] == dgp) & (results_partA[\"S_true\"] == float(s_true))]\n",
    "        for method in method_order:\n",
    "            s = sub[sub[\"method\"] == method].sort_values(\"n\")\n",
    "            y = s[\"reject_rate_H0_S_le_0\"].to_numpy(dtype=float)\n",
    "            if np.isfinite(y).any():\n",
    "                ax.plot(s[\"n\"], y, marker=\"o\", label=method)\n",
    "\n",
    "        ax.set_title(f\"{dgp} | S_true={s_true}\")\n",
    "        ax.set_xlabel(\"n\")\n",
    "        ax.set_ylabel(\"reject_rate_H0_S_le_0\")\n",
    "        ax.set_xticks(CONFIG[\"n_list\"])\n",
    "        ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    if handles:\n",
    "        fig.legend(handles, labels, loc=\"upper center\", ncol=min(4, len(labels)))\n",
    "    fig.suptitle(f\"Part A Reject Rate vs n ({dgp})\", y=1.05)\n",
    "    fig.tight_layout()\n",
    "    save_figure(fig, f\"partA_reject_vs_n_{dgp}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"Saved Part A reject-rate figures.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B: load data and split into train/holdout\n",
    "partB_status = {\n",
    "    \"run\": False,\n",
    "    \"reason\": \"\",\n",
    "}\n",
    "\n",
    "partB_window_records = []\n",
    "partB_diag_df = pd.DataFrame()\n",
    "partB_results = pd.DataFrame()\n",
    "model_fit_summary = {}\n",
    "\n",
    "PARTB_N = 240\n",
    "\n",
    "\n",
    "def _load_excess_return_series(path: Path) -> pd.Series:\n",
    "    if path.suffix.lower() in {\".parquet\", \".pq\"}:\n",
    "        obj = pd.read_parquet(path)\n",
    "    else:\n",
    "        obj = pd.read_csv(path)\n",
    "\n",
    "    if isinstance(obj, pd.Series):\n",
    "        s = obj\n",
    "    elif isinstance(obj, pd.DataFrame):\n",
    "        num_cols = list(obj.select_dtypes(include=[np.number]).columns)\n",
    "        if not num_cols:\n",
    "            raise ValueError(\"No numeric return column found in input file.\")\n",
    "        preferred = [c for c in num_cols if c.lower() in {\"excess_return\", \"excess\", \"rx\", \"ret_excess\", \"return\"}]\n",
    "        use_col = preferred[0] if preferred else num_cols[0]\n",
    "        s = obj[use_col]\n",
    "\n",
    "        # If a date-like column exists, use it as index for friendlier window labels.\n",
    "        date_candidates = [c for c in obj.columns if \"date\" in c.lower() or \"month\" in c.lower()]\n",
    "        if date_candidates:\n",
    "            try:\n",
    "                idx = pd.to_datetime(obj[date_candidates[0]], errors=\"coerce\")\n",
    "                if idx.notna().sum() >= int(0.7 * len(obj)):\n",
    "                    s = pd.Series(s.to_numpy(), index=idx)\n",
    "            except Exception:\n",
    "                pass\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported data object type.\")\n",
    "\n",
    "    s = pd.to_numeric(s, errors=\"coerce\").dropna()\n",
    "    return s.astype(float)\n",
    "\n",
    "\n",
    "data_path = Path(CONFIG[\"data_path\"])\n",
    "if not data_path.exists():\n",
    "    partB_status[\"reason\"] = f\"Skipped Part B: data_path not found -> {data_path}\"\n",
    "    print(partB_status[\"reason\"])\n",
    "else:\n",
    "    try:\n",
    "        returns = _load_excess_return_series(data_path)\n",
    "    except Exception as e:\n",
    "        partB_status[\"reason\"] = f\"Skipped Part B: failed to load/parse data ({e})\"\n",
    "        print(partB_status[\"reason\"])\n",
    "        returns = None\n",
    "\n",
    "    if returns is not None:\n",
    "        T = int(len(returns))\n",
    "        if T < 2 * PARTB_N:\n",
    "            partB_status[\"reason\"] = f\"Skipped Part B: need at least {2 * PARTB_N} observations, got {T}.\"\n",
    "            print(partB_status[\"reason\"])\n",
    "        else:\n",
    "            holdout_len = 360 if T >= 1200 else max(PARTB_N, int(round(0.30 * T)))\n",
    "            train_len = T - holdout_len\n",
    "            if train_len < PARTB_N:\n",
    "                partB_status[\"reason\"] = (\n",
    "                    f\"Skipped Part B: training length too short ({train_len}) after split; \"\n",
    "                    f\"T={T}, holdout_len={holdout_len}.\"\n",
    "                )\n",
    "                print(partB_status[\"reason\"])\n",
    "            else:\n",
    "                train = returns.iloc[:train_len].copy()\n",
    "                holdout = returns.iloc[train_len:].copy()\n",
    "                n_windows = int(len(holdout) // PARTB_N)\n",
    "                if n_windows < 1:\n",
    "                    partB_status[\"reason\"] = (\n",
    "                        f\"Skipped Part B: holdout has {len(holdout)} points, no disjoint window of n={PARTB_N}.\"\n",
    "                    )\n",
    "                    print(partB_status[\"reason\"])\n",
    "                else:\n",
    "                    partB_status[\"run\"] = True\n",
    "                    partB_status[\"reason\"] = \"Part B ready.\"\n",
    "                    holdout_use = holdout.iloc[: n_windows * PARTB_N]\n",
    "\n",
    "                    for j in range(n_windows):\n",
    "                        w = holdout_use.iloc[j * PARTB_N : (j + 1) * PARTB_N]\n",
    "                        s_obs, _, _ = sharpe_mc.sharpe_ratio(w.to_numpy(dtype=np.float64))\n",
    "\n",
    "                        start_label = w.index[0] if len(w.index) else j * PARTB_N\n",
    "                        end_label = w.index[-1] if len(w.index) else (j + 1) * PARTB_N - 1\n",
    "\n",
    "                        partB_window_records.append(\n",
    "                            {\n",
    "                                \"window_id\": j,\n",
    "                                \"window_start\": str(start_label),\n",
    "                                \"window_end\": str(end_label),\n",
    "                                \"S_obs\": float(s_obs),\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                    partB_window_obs = pd.DataFrame(partB_window_records)\n",
    "                    print(\n",
    "                        f\"Part B data loaded: T={T}, train_len={train_len}, holdout_len={len(holdout)}, \"\n",
    "                        f\"disjoint_windows={n_windows}, n_window={PARTB_N}.\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B: fit 3 models, bootstrap Sharpe CDFs, compute PIT and diagnostics, save artifacts\n",
    "partB_diag_rows = []\n",
    "partB_window_rows = []\n",
    "\n",
    "if not partB_status[\"run\"]:\n",
    "    partB_results = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"model\",\n",
    "            \"window_id\",\n",
    "            \"window_start\",\n",
    "            \"window_end\",\n",
    "            \"S_obs\",\n",
    "            \"pit_u\",\n",
    "            \"ks_stat\",\n",
    "            \"ks_pvalue\",\n",
    "            \"pit_score\",\n",
    "            \"n_windows\",\n",
    "            \"n_boot\",\n",
    "        ]\n",
    "    )\n",
    "    model_fit_summary = {\"status\": \"skipped\", \"reason\": partB_status[\"reason\"]}\n",
    "else:\n",
    "    train_arr = train.to_numpy(dtype=np.float64)\n",
    "    train_sig = series_checksum(train_arr)\n",
    "    Rb = int(CONFIG[\"R\"])\n",
    "\n",
    "    model_names = [\"iid_normal\", \"iid_t_fixed_nu\", \"garch11_t_fixed_nu_approx\"]\n",
    "    spawned = np.random.SeedSequence(int(CONFIG[\"seed\"]) + 20260218).spawn(len(model_names))\n",
    "\n",
    "    for model_idx, model_name in enumerate(model_names):\n",
    "        model_seed = int(spawned[model_idx].generate_state(1)[0])\n",
    "        rng = np.random.default_rng(model_seed)\n",
    "\n",
    "        cache_stem = (\n",
    "            f\"partB_bootstrap_Shat_{model_name}_R{Rb}_n{PARTB_N}_\"\n",
    "            f\"seed{model_seed}_nu{_safe_tag(CONFIG['t_df'])}_\"\n",
    "            f\"burn{int(CONFIG['burn_B'])}_train{len(train_arr)}_{train_sig}\"\n",
    "        )\n",
    "\n",
    "        sorted_sr = cache_load_array(cache_stem)\n",
    "        fit_info = {\"model\": model_name, \"seed\": model_seed, \"cached\": sorted_sr is not None}\n",
    "\n",
    "        if sorted_sr is None:\n",
    "            sr_boot = np.empty(Rb, dtype=np.float64)\n",
    "\n",
    "            if model_name == \"iid_normal\":\n",
    "                mu_hat = float(np.mean(train_arr))\n",
    "                sigma_hat = float(np.std(train_arr, ddof=1))\n",
    "                fit_info.update({\"mu_hat\": mu_hat, \"sigma_hat\": sigma_hat})\n",
    "\n",
    "                batch = 512\n",
    "                ptr = 0\n",
    "                for start in tqdm(range(0, Rb, batch), desc=f\"Part B bootstrap {model_name}\", leave=False):\n",
    "                    m = min(batch, Rb - start)\n",
    "                    draws = mu_hat + sigma_hat * rng.standard_normal(size=(m, PARTB_N))\n",
    "                    means = draws.mean(axis=1)\n",
    "                    stds = draws.std(axis=1, ddof=1)\n",
    "                    sr_boot[ptr : ptr + m] = means / stds\n",
    "                    ptr += m\n",
    "\n",
    "            elif model_name == \"iid_t_fixed_nu\":\n",
    "                nu = float(CONFIG[\"t_df\"])\n",
    "                scale = np.sqrt(nu / (nu - 2.0))\n",
    "                mu_hat = float(np.mean(train_arr))\n",
    "                sigma_hat = float(np.std(train_arr, ddof=1))\n",
    "                fit_info.update({\"mu_hat\": mu_hat, \"sigma_hat\": sigma_hat, \"nu_fixed\": nu})\n",
    "\n",
    "                batch = 512\n",
    "                ptr = 0\n",
    "                for start in tqdm(range(0, Rb, batch), desc=f\"Part B bootstrap {model_name}\", leave=False):\n",
    "                    m = min(batch, Rb - start)\n",
    "                    z = rng.standard_t(df=nu, size=(m, PARTB_N)) / scale\n",
    "                    draws = mu_hat + sigma_hat * z\n",
    "                    means = draws.mean(axis=1)\n",
    "                    stds = draws.std(axis=1, ddof=1)\n",
    "                    sr_boot[ptr : ptr + m] = means / stds\n",
    "                    ptr += m\n",
    "\n",
    "            elif model_name == \"garch11_t_fixed_nu_approx\":\n",
    "                # Closest project-native approach: fit garch11_t with project fitter (nu estimated),\n",
    "                # then set nu to CONFIG['t_df'] for simulation if fixed-nu fitting is unavailable.\n",
    "                model_obj, fit_res, params_hat = sharpe_mc.fit_candidate(train_arr, \"garch11_t\")\n",
    "                params_sim = np.asarray(params_hat, dtype=np.float64).copy()\n",
    "                nu_hat = float(params_sim[-1])\n",
    "                params_sim[-1] = float(CONFIG[\"t_df\"])\n",
    "                initial_var = float(max(np.var(train_arr, ddof=1), 1e-8))\n",
    "\n",
    "                fit_info.update(\n",
    "                    {\n",
    "                        \"mu_hat\": float(params_sim[0]),\n",
    "                        \"omega_hat\": float(params_sim[1]),\n",
    "                        \"alpha_hat\": float(params_sim[2]),\n",
    "                        \"beta_hat\": float(params_sim[3]),\n",
    "                        \"nu_hat_from_fit\": nu_hat,\n",
    "                        \"nu_used_in_sim\": float(params_sim[-1]),\n",
    "                        \"note\": (\n",
    "                            \"Fixed-nu optimization is not directly exposed by the project fitter; \"\n",
    "                            \"used closest approach: estimate params with project garch11_t fitter, \"\n",
    "                            \"then overwrite nu in simulation.\"\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # arch simulation inside sharpe_mc uses global np RNG\n",
    "                set_global_seed(model_seed)\n",
    "                for i in tqdm(range(Rb), desc=f\"Part B bootstrap {model_name}\", leave=False):\n",
    "                    sim = sharpe_mc.simulate_from_fit(\n",
    "                        model_obj,\n",
    "                        params_sim,\n",
    "                        n=PARTB_N,\n",
    "                        burn=int(CONFIG[\"burn_B\"]),\n",
    "                        initial_value_vol=initial_var,\n",
    "                    )\n",
    "                    s_hat_i, _, _ = sharpe_mc.sharpe_ratio(sim)\n",
    "                    sr_boot[i] = float(s_hat_i)\n",
    "            else:\n",
    "                raise ValueError(model_name)\n",
    "\n",
    "            sr_boot = sr_boot[np.isfinite(sr_boot)]\n",
    "            if sr_boot.size == 0:\n",
    "                raise RuntimeError(f\"No finite bootstrap Sharpe draws for model {model_name}.\")\n",
    "            sorted_sr = np.sort(sr_boot)\n",
    "            cache_save_array(sorted_sr, cache_stem)\n",
    "\n",
    "        model_fit_summary[model_name] = fit_info\n",
    "\n",
    "        # PIT for each disjoint holdout window\n",
    "        u_vals = []\n",
    "        for _, wrow in partB_window_obs.iterrows():\n",
    "            s_obs = float(wrow[\"S_obs\"])\n",
    "            u = float(np.searchsorted(sorted_sr, s_obs, side=\"right\") / sorted_sr.size)\n",
    "            u_vals.append(u)\n",
    "            partB_window_rows.append(\n",
    "                {\n",
    "                    \"model\": model_name,\n",
    "                    \"window_id\": int(wrow[\"window_id\"]),\n",
    "                    \"window_start\": wrow[\"window_start\"],\n",
    "                    \"window_end\": wrow[\"window_end\"],\n",
    "                    \"S_obs\": s_obs,\n",
    "                    \"pit_u\": u,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        u_arr = np.asarray(u_vals, dtype=np.float64)\n",
    "        if u_arr.size == 0:\n",
    "            ks_stat, ks_pvalue, pit_score = np.nan, np.nan, np.nan\n",
    "        else:\n",
    "            ks = stats.kstest(u_arr, \"uniform\")\n",
    "            ks_stat = float(ks.statistic)\n",
    "            ks_pvalue = float(ks.pvalue)\n",
    "            pit_score = float(np.mean(np.abs(u_arr - 0.5)))\n",
    "\n",
    "        partB_diag_rows.append(\n",
    "            {\n",
    "                \"model\": model_name,\n",
    "                \"ks_stat\": ks_stat,\n",
    "                \"ks_pvalue\": ks_pvalue,\n",
    "                \"pit_score\": pit_score,\n",
    "                \"n_windows\": int(u_arr.size),\n",
    "                \"n_boot\": int(sorted_sr.size),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    partB_window_df = pd.DataFrame(partB_window_rows)\n",
    "    partB_diag_df = pd.DataFrame(partB_diag_rows)\n",
    "    partB_results = partB_window_df.merge(partB_diag_df, on=\"model\", how=\"left\")\n",
    "\n",
    "# Save Part B outputs (including skipped placeholder if skipped)\n",
    "partB_paths = result_save_df(partB_results, \"results_partB_pit\")\n",
    "fit_summary_path = RESULTS_DIR / \"model_fit_summary.json\"\n",
    "fit_summary_path.write_text(json.dumps(model_fit_summary, indent=2), encoding=\"utf-8\")\n",
    "_track_artifact(fit_summary_path)\n",
    "\n",
    "print(\"Saved Part B results:\")\n",
    "for p in partB_paths:\n",
    "    print(\"-\", p)\n",
    "print(\"-\", fit_summary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B diagnostics table\n",
    "if partB_status[\"run\"]:\n",
    "    print(\"Part B diagnostics:\")\n",
    "    print(partB_diag_df)\n",
    "else:\n",
    "    print(partB_status[\"reason\"])\n",
    "\n",
    "print(\"\\nPart B window-level head:\")\n",
    "print(partB_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B plots: PIT histograms per model\n",
    "if partB_results.empty:\n",
    "    print(\"Skipping PIT histogram plots because Part B results are empty.\")\n",
    "else:\n",
    "    for model_name, sub in partB_results.groupby(\"model\"):\n",
    "        u = sub[\"pit_u\"].to_numpy(dtype=np.float64)\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        ax.hist(u, bins=np.linspace(0.0, 1.0, 11), density=True, alpha=0.75, edgecolor=\"black\")\n",
    "        ax.axhline(1.0, color=\"black\", linestyle=\"--\", linewidth=1.0)\n",
    "        ax.set_xlim(0.0, 1.0)\n",
    "        ax.set_xlabel(\"PIT u\")\n",
    "        ax.set_ylabel(\"density\")\n",
    "        ax.set_title(f\"PIT histogram | {model_name}\")\n",
    "        fig.tight_layout()\n",
    "        save_figure(fig, f\"partB_pit_hist_{model_name}\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(\"Saved Part B PIT histogram figures.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B plots: ECDF(u) vs uniform line per model\n",
    "if partB_results.empty:\n",
    "    print(\"Skipping PIT ECDF plots because Part B results are empty.\")\n",
    "else:\n",
    "    for model_name, sub in partB_results.groupby(\"model\"):\n",
    "        u = np.sort(sub[\"pit_u\"].to_numpy(dtype=np.float64))\n",
    "        m = u.size\n",
    "        y = np.arange(1, m + 1, dtype=np.float64) / m\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        ax.step(u, y, where=\"post\", label=\"ECDF(u)\")\n",
    "        ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\", label=\"Uniform(0,1)\")\n",
    "        ax.set_xlim(0.0, 1.0)\n",
    "        ax.set_ylim(0.0, 1.0)\n",
    "        ax.set_xlabel(\"u\")\n",
    "        ax.set_ylabel(\"ECDF\")\n",
    "        ax.set_title(f\"PIT ECDF vs Uniform | {model_name}\")\n",
    "        ax.legend(loc=\"lower right\")\n",
    "        fig.tight_layout()\n",
    "        save_figure(fig, f\"partB_pit_ecdf_{model_name}\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(\"Saved Part B ECDF figures.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final run summary\n",
    "print(\"Part A results (head):\")\n",
    "print(results_partA.head())\n",
    "\n",
    "print(\"\\nPart B results (head):\")\n",
    "print(partB_results.head())\n",
    "\n",
    "print(\"\\nSaved artifacts:\")\n",
    "for p in sorted(set(SAVED_ARTIFACTS)):\n",
    "    print(\"-\", p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
